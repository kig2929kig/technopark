{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YJF6V6_KkL95"},"outputs":[],"source":["!pip install -U -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"u44DsECKh9VA"},"source":["## 학습 전 모델로 MBTI 분류해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUpX6BJ8h316"},"outputs":[],"source":["import torch\n","from transformers import pipeline, AutoModelForCausalLM\n","\n","MODEL = 'EleutherAI/polyglot-ko-1.3b' #1.3b, 3.8b, 5.8b, 12.8b\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL,\n","    torch_dtype=torch.float16,\n","    low_cpu_mem_usage=True,\n",").to(device=f\"cuda\", non_blocking=True)\n","model.eval()\n","\n","pipe = pipeline(\n","    'text-generation',\n","    model=model,\n","    tokenizer=MODEL,\n","    device=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtphis3AiA26"},"outputs":[],"source":["def answer(state, state_chatbot, text):\n","    messages = state + [{\"role\": \"질문\", \"content\": text}]\n","\n","    conversation_history = \"\\n\".join(\n","        [f\"### {msg['role']}:\\n{msg['content']}\" for msg in messages]\n","    )\n","\n","    ans = pipe(\n","        conversation_history + \"\\n\\n### 답변:\",\n","        do_sample=True,\n","        max_new_tokens=64,\n","        temperature=0.7,\n","        top_p=0.9,\n","        return_full_text=False,\n","        eos_token_id=2,\n","    )\n","\n","    msg = ans[0][\"generated_text\"]\n","\n","    if \"###\" in msg:\n","        msg = msg.split(\"###\")[0]\n","\n","    new_state = [{\"role\": \"이전 질문\", \"content\": text}, {\"role\": \"이전 답변\", \"content\": msg}]\n","\n","    state = state + new_state\n","    state_chatbot = state_chatbot + [(text, msg)]\n","\n","    return state, state_chatbot, state_chatbot, msg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcFVdB96iDFL"},"outputs":[],"source":["state = [\n","           {\n","                \"role\": \"맥락\",\n","                \"content\": \"KoAlpaca(코알파카)는 EleutherAI에서 개발한 Polyglot-ko 라는 한국어 모델을 기반으로, 자연어 처리 연구자 Beomi가 개발한 모델입니다.\",\n","            },\n","            {\n","                \"role\": \"맥락\",\n","                \"content\": \"ChatKoAlpaca(챗코알파카)는 KoAlpaca를 채팅형으로 만든 것입니다.\",\n","            },\n","            {\"role\": \"명령어\", \"content\": \"친절한 AI 챗봇인 ChatKoAlpaca 로서 답변을 합니다.\"},\n","            {\n","                \"role\": \"명령어\",\n","                \"content\": \"인사에는 짧고 간단한 친절한 인사로 답하고, 아래 대화에 간단하고 짧게 답해주세요.\",\n","            },\n","    ]\n","\n","state_chatbot = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1zqIWZDiE18"},"outputs":[],"source":["while True:\n","  text = input(\"질문: \")\n","\n","  if text == '그만':\n","    break\n","\n","  state, state_chatbot, _, msg = answer(state, state_chatbot,text)\n","  print(\"답변: \", msg)\n"]},{"cell_type":"markdown","metadata":{"id":"pPSKHvpCG4xS"},"source":["## MBTI 질문 데이터로 모델 학습 시키기"]},{"cell_type":"markdown","metadata":{"id":"1cVrWnXhkoyK"},"source":["###!!!!이 코드 실행 전 런타임 다시 시작 눌러주세요!!!!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDKAGbyjkRAi"},"outputs":[],"source":["!python run_clm.py \\\n","--model_name_or_path='EleutherAI/polyglot-ko-1.3b' \\\n","--train_file='./mbti_0920_small.json' \\\n","--num_train_epochs=4 \\\n","--block_size=256 \\\n","--per_device_train_batch_size=1 \\\n","--gradient_accumulation_steps=8 \\\n","--fp16 \\\n","--output_dir='my_model' \\\n","--do_train \\\n","--optim='adafactor' \\\n","--learning_rate='2e-5' \\\n","--logging_strategy='steps' \\\n","--logging_first_step \\\n","--run_name='polyglot-1.3b-koalpaca-v1.1a-rtx3090' \\\n","--low_cpu_mem_usage"]},{"cell_type":"markdown","metadata":{"id":"p6dg3CZsG-HQ"},"source":["## 학습된 모델로 질문해보기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMk8QyfXk3eu"},"outputs":[],"source":["import torch\n","from transformers import pipeline, AutoModelForCausalLM\n","\n","MODEL = '/content/my_model'\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL,\n","    torch_dtype=torch.float16,\n","    low_cpu_mem_usage=True,\n",").to(device=f\"cuda\", non_blocking=True)\n","model.eval()\n","\n","pipe = pipeline(\n","    'text-generation',\n","    model=model,\n","    tokenizer=MODEL,\n","    device=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtPLMB3SCeSO"},"outputs":[],"source":["def answer(state, state_chatbot, text):\n","    messages = state + [{\"role\": \"질문\", \"content\": text}]\n","\n","    # print('messages: ', messages)\n","    # print('state: ', state)\n","    # print('text:', text)\n","\n","    conversation_history = \"\\n\".join(\n","        [f\"### {msg['role']}:\\n{msg['content']}\" for msg in messages]\n","    )\n","\n","    ans = pipe(\n","        conversation_history + \"\\n\\n### 답변:\",\n","        do_sample=True,\n","        max_new_tokens=64,\n","        temperature=0.7,\n","        top_p=0.9,\n","        return_full_text=False,\n","        eos_token_id=2,\n","    )\n","\n","    msg = ans[0][\"generated_text\"]\n","\n","    # print(\"msg: \", msg)\n","\n","    if \"###\" in msg:\n","        msg = msg.split(\"###\")[0]\n","\n","    new_state = [{\"role\": \"이전 질문\", \"content\": text}, {\"role\": \"이전 답변\", \"content\": msg}]\n","\n","    state = state + new_state\n","    state_chatbot = state_chatbot + [(text, msg)]\n","\n","    # print(state)\n","    # print(state_chatbot)\n","\n","    return state, state_chatbot, msg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0tNOXz0Ddyb"},"outputs":[],"source":["state = [\n","            {\n","                \"role\": \"맥락\",\n","                \"content\": \"MBTI 분류 챗봇입니다.\",\n","            },\n","            {\n","                \"role\": \"명령어\",\n","                \"content\": \"사람들의 답변을 기반으로 MBTI를 맞힙니다. \"},\n","    ]\n","\n","state_chatbot = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f6gMRIiDxV8"},"outputs":[],"source":["while True:\n","  text = input(\"질문: \")\n","\n","  if text == '그만':\n","    break\n","\n","  state, state_chatbot, msg = answer(state, state_chatbot,text)\n","  print(\"답변: \", msg)\n"]},{"cell_type":"markdown","metadata":{"id":"wpMAg9sEK_gz"},"source":["## 학습한 모델 실제 챗봇 서비스로 실행시키기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"681QipVCKRuY"},"outputs":[],"source":["def answer(state, state_chatbot, text):\n","    messages = state + [{\"role\": \"질문\", \"content\": text}]\n","\n","    conversation_history = \"\\n\".join(\n","        [f\"### {msg['role']}:\\n{msg['content']}\" for msg in messages]\n","    )\n","\n","    ans = pipe(\n","        conversation_history + \"\\n\\n### 답변:\",\n","        do_sample=True,\n","        max_new_tokens=64,\n","        temperature=0.7,\n","        top_p=0.9,\n","        return_full_text=False,\n","        eos_token_id=2,\n","    )\n","\n","    msg = ans[0][\"generated_text\"]\n","\n","    if \"###\" in msg:\n","        msg = msg.split(\"###\")[0]\n","\n","    new_state = [{\"role\": \"이전 질문\", \"content\": text}, {\"role\": \"이전 답변\", \"content\": msg}]\n","\n","    state = state + new_state\n","    state_chatbot = state_chatbot + [(text, msg)]\n","\n","    print(state)\n","    print(state_chatbot)\n","\n","    return state, state_chatbot, state_chatbot\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnNb-lhZHK0w"},"outputs":[],"source":["import gradio as gr\n","\n","with gr.Blocks(css=\"#chatbot .overflow-y-auto{height:750px}\") as demo:\n","    state = gr.State(\n","        [\n","           {\n","                \"role\": \"맥락\",\n","                \"content\": \"MBTI 분류 챗봇입니다.\",\n","            },\n","            {\n","                \"role\": \"명령어\",\n","                \"content\": \"사람들의 답변을 기반으로 MBTI를 맞힙니다. \"},\n","        ]\n","    )\n","    state_chatbot = gr.State([])\n","\n","    with gr.Row():\n","        gr.HTML(\n","            \"\"\"<div style=\"text-align: center; max-width: 500px; margin: 0 auto;\">\n","            <div>\n","                <h1>나만의 MBTI 분류기</h1>\n","            </div>\n","            <div>\n","                Demo runs on RTX 3090 (24GB) with 8bit quantized\n","            </div>\n","        </div>\"\"\"\n","        )\n","\n","    with gr.Row():\n","        chatbot = gr.Chatbot(elem_id=\"chatbot\")\n","\n","    with gr.Row():\n","        txt = gr.Textbox(show_label=False, placeholder=\"Send a message...\")\n","\n","    txt.submit(answer, [state, state_chatbot, txt], [state, state_chatbot, chatbot])\n","    txt.submit(lambda: \"\", None, txt)\n","\n","demo.queue().launch(debug=True, server_name=\"0.0.0.0\", share=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ouab5VjUo8a7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}